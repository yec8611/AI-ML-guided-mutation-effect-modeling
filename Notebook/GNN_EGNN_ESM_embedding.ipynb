{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN only model\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.spatial import KDTree\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from Bio.PDB import PDBParser, Polypeptide\n",
    "\n",
    "def get_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    return \"cpu\"\n",
    "\n",
    "OPTIMAL_RADIUS = 8.0\n",
    "OPTIMAL_HIDDEN_DIM = 96\n",
    "OPTIMAL_DROPOUT = 0.4\n",
    "\n",
    "SEED = 42\n",
    "N_SPLITS = 5          \n",
    "BATCH_SIZE = 16\n",
    "MAX_EPOCHS = 50       \n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "EARLY_STOPPING_PATIENCE = 15\n",
    "\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "\n",
    "mutation_data_path = 'mutations_full_info.pkl' # data stored in pickle format, with columns: mutation, scaled_activity, group, sequence\n",
    "mutations = pd.read_pickle(mutation_data_path)\n",
    "\n",
    "y = mutations[\"scaled_activity\"].to_numpy(dtype=np.float32)\n",
    "groups = mutations[\"mutation\"].str.extract(r\"[A-Z](\\d+)[A-Z]\").astype(int)[0].to_numpy()\n",
    "\n",
    "def custom_three_to_one(residue_name: str) -> str:\n",
    "    _3to1_map = {'ALA': 'A', 'CYS': 'C', 'ASP': 'D', 'GLU': 'E', 'PHE': 'F', 'GLY': 'G', 'HIS': 'H', 'ILE': 'I', 'LYS': 'K', 'LEU': 'L', 'MET': 'M', 'ASN': 'N', 'PRO': 'P', 'GLN': 'Q', 'ARG': 'R', 'SER': 'S', 'THR': 'T', 'VAL': 'V', 'TRP': 'W', 'TYR': 'Y'}\n",
    "    return _3to1_map.get(residue_name.upper(), 'X')\n",
    "\n",
    "WT_PDB_PATH = \"wt_af.pdb\" # wild-type protein structure in pdb format predicted by AlphaFold2\n",
    "parser = PDBParser(QUIET=True)\n",
    "structure = parser.get_structure(\"wt\", WT_PDB_PATH)\n",
    "chain = next(structure.get_chains())\n",
    "residues = [res for res in chain.get_residues() if Polypeptide.is_aa(res, standard=True)]\n",
    "coords = np.array([res[\"CA\"].get_coord() for res in residues], dtype=np.float32)\n",
    "wt_seq = \"\".join(custom_three_to_one(res.get_resname()) for res in residues)\n",
    "\n",
    "AA_CODES = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "def aa_onehot(code):\n",
    "    vec = np.zeros(len(AA_CODES), dtype=np.float32)\n",
    "    vec[AA_CODES.index(code)] = 1.0\n",
    "    return vec\n",
    "\n",
    "BASE_NODE_FEATURES = np.stack([aa_onehot(a) for a in wt_seq])\n",
    "\n",
    "all_node_features = []\n",
    "mut_pat = re.compile(r\"([A-Z])(\\d+)([A-Z])\")\n",
    "for _, row in mutations.iterrows():\n",
    "    mut = row[\"mutation\"]\n",
    "    match = mut_pat.fullmatch(mut)\n",
    "    if not match: continue\n",
    "    \n",
    "    wt, pos, mut_aa = match.groups()\n",
    "    idx = int(pos) - 1\n",
    "    \n",
    "    x = BASE_NODE_FEATURES.copy()\n",
    "    mutation_indicator = np.zeros((len(wt_seq), 1), dtype=np.float32)\n",
    "    mutation_indicator[idx] = 1.0\n",
    "    mut_aa_onehot = np.zeros((len(wt_seq), len(AA_CODES)), dtype=np.float32)\n",
    "    mut_aa_onehot[idx] = aa_onehot(mut_aa)\n",
    "    \n",
    "    combined_features = np.hstack([x, mutation_indicator, mut_aa_onehot])\n",
    "    all_node_features.append(combined_features)\n",
    "\n",
    "\n",
    "\n",
    "def create_radius_adjacency_matrix(coords, radius=10.0):\n",
    "    tree = KDTree(coords)\n",
    "    pairs = tree.query_pairs(r=radius)\n",
    "    row_indices, col_indices = zip(*pairs) if pairs else ([], [])\n",
    "    \n",
    "    all_rows = np.concatenate([row_indices, col_indices, np.arange(len(coords))])\n",
    "    all_cols = np.concatenate([col_indices, row_indices, np.arange(len(coords))])\n",
    "    \n",
    "    adj = coo_matrix((np.ones(len(all_rows)), (all_rows, all_cols)), shape=(len(coords), len(coords)))\n",
    "    return adj.tocsr()\n",
    "\n",
    "\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout=0.2, alpha=0.2):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.zeros(size=(2 * out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
    "\n",
    "    def forward(self, h, adj):\n",
    "        Wh = torch.mm(h, self.W)\n",
    "        a_input = self._prepare_attentional_mechanism_input(Wh)\n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
    "\n",
    "        zero_vec = -9e15 * torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.matmul(attention, Wh)\n",
    "        return F.elu(h_prime)\n",
    "\n",
    "    def _prepare_attentional_mechanism_input(self, Wh):\n",
    "        N = Wh.size()[0]\n",
    "        Wh_repeated_in_chunks = Wh.repeat_interleave(N, dim=0)\n",
    "        Wh_repeated_alternating = Wh.repeat(N, 1)\n",
    "        all_combinations_matrix = torch.cat([Wh_repeated_in_chunks, Wh_repeated_alternating], dim=1)\n",
    "        return all_combinations_matrix.view(N, N, 2 * self.out_features)\n",
    "\n",
    "class MutGraphNet(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.gat1 = GraphAttentionLayer(in_dim, hidden_dim, dropout=dropout)\n",
    "        self.gat2 = GraphAttentionLayer(hidden_dim, hidden_dim, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = self.gat1(x, adj)\n",
    "        x = self.dropout(x)\n",
    "        x = self.gat2(x, adj)\n",
    "        x = x.mean(dim=0)\n",
    "        return self.fc(x)\n",
    "\n",
    "class ProteinGraphDataset(Dataset):\n",
    "    def __init__(self, features_list, labels):\n",
    "        self.features_list = features_list\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.features_list[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate_gnn(features, labels, groups, adj, params, patience):\n",
    "    kf = GroupKFold(n_splits=N_SPLITS)\n",
    "    oof_preds = np.zeros(len(labels))\n",
    "    device = get_device()\n",
    "\n",
    "\n",
    "    adj_torch = torch.from_numpy(adj.todense().astype(np.float32)).to(device)\n",
    "\n",
    "    fold_metrics_history = {\n",
    "        'rmse': [], 'r2': [], 'pearson': [], 'spearman': []\n",
    "    }\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(features, labels, groups)):\n",
    "\n",
    "        model = MutGraphNet(\n",
    "            in_dim=features[0].shape[1],\n",
    "            hidden_dim=params['hidden_dim'],\n",
    "            dropout=params['dropout']\n",
    "        ).to(device)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'], weight_decay=1e-5)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5)\n",
    "\n",
    "        train_dataset = ProteinGraphDataset([features[i] for i in train_idx], labels[train_idx])\n",
    "        val_dataset = ProteinGraphDataset([features[i] for i in val_idx], labels[val_idx])\n",
    "        loader_tr = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        loader_va = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        best_model_path = f\"best_model_fold_{fold+1}.pth\"\n",
    "\n",
    "        for epoch in range(MAX_EPOCHS):\n",
    "            model.train()\n",
    "            total_train_loss = 0\n",
    "            for x_batch, y_batch in loader_tr:\n",
    "                for i in range(x_batch.size(0)):\n",
    "                    x, y = x_batch[i].to(device), y_batch[i].to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    output = model(x, adj_torch).squeeze()\n",
    "                    loss = F.mse_loss(output, y)\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    optimizer.step()\n",
    "                    total_train_loss += loss.item()\n",
    "            avg_train_loss = total_train_loss / len(train_dataset)\n",
    "\n",
    "            model.eval()\n",
    "            total_val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for x_batch, y_batch in loader_va:\n",
    "                    for i in range(x_batch.size(0)):\n",
    "                        x, y = x_batch[i].to(device), y_batch[i].to(device)\n",
    "                        output = model(x, adj_torch).squeeze()\n",
    "                        total_val_loss += F.mse_loss(output, y).item()\n",
    "            avg_val_loss = total_val_loss / len(val_dataset)\n",
    "            scheduler.step(avg_val_loss)\n",
    "\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                patience_counter = 0\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}.\")\n",
    "                break\n",
    "        \n",
    "        model.load_state_dict(torch.load(best_model_path))\n",
    "        model.eval()\n",
    "        fold_preds_list = []\n",
    "        fold_true_labels = labels[val_idx]\n",
    "        with torch.no_grad():\n",
    "            for x_batch, _ in loader_va:\n",
    "                 for i in range(x_batch.size(0)):\n",
    "                    x = x_batch[i].to(device)\n",
    "                    fold_preds_list.append(model(x, adj_torch).squeeze().cpu().numpy())\n",
    "        \n",
    "        fold_preds = np.array(fold_preds_list).flatten()\n",
    "        oof_preds[val_idx] = fold_preds\n",
    "\n",
    "        fold_metrics_history['rmse'].append(np.sqrt(mean_squared_error(fold_true_labels, fold_preds)))\n",
    "        fold_metrics_history['r2'].append(r2_score(fold_true_labels, fold_preds))\n",
    "        fold_metrics_history['pearson'].append(pearsonr(fold_true_labels, fold_preds)[0])\n",
    "        fold_metrics_history['spearman'].append(spearmanr(fold_true_labels, fold_preds)[0])\n",
    "        \n",
    "        print(f\"Fold {fold+1} spearman: {fold_metrics_history['spearman'][-1]:.4f}\")\n",
    "        os.remove(best_model_path)\n",
    "\n",
    "    overall_metrics = {\n",
    "        'rmse': np.sqrt(mean_squared_error(labels, oof_preds)),\n",
    "        'r2': r2_score(labels, oof_preds),\n",
    "        'pearson': pearsonr(labels, oof_preds)[0],\n",
    "        'spearman': spearmanr(labels, oof_preds)[0]\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'predictions': oof_preds,\n",
    "        'overall_metrics': overall_metrics,\n",
    "        'fold_metrics': fold_metrics_history\n",
    "    }\n",
    "\n",
    "\n",
    "adj_matrix = create_radius_adjacency_matrix(coords, radius=OPTIMAL_RADIUS)\n",
    "model_params = {'lr': LEARNING_RATE, 'hidden_dim': OPTIMAL_HIDDEN_DIM, 'dropout': OPTIMAL_DROPOUT}\n",
    "final_results = train_and_evaluate_gnn(\n",
    "    all_node_features, y, groups, adj=adj_matrix, params=model_params, patience=EARLY_STOPPING_PATIENCE\n",
    ")\n",
    "\n",
    "\n",
    "for metric_name, metric_values in final_results['fold_metrics'].items():\n",
    "    mean_val = np.mean(metric_values)\n",
    "    std_val = np.std(metric_values)\n",
    "    \n",
    "\n",
    "overall_metrics = final_results['overall_metrics']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate embeddings (ESM-2-650M) for all mutants\n",
    "import os, re, json, numpy as np, pandas as pd, torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from Bio.PDB import PDBParser, Polypeptide\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "DF_PATH      = \"mutations_full_info.pkl\" # data stored in pickle format, with columns: mutation, scaled_activity, group, sequence\n",
    "OUT_PATH     = \"mutant_embeddings.npz\"   # save embeddings in npz format\n",
    "MODEL_NAME   = \"facebook/esm2_t33_650M_UR50D\"\n",
    "BATCH_SIZE   = 16                                 \n",
    "dtype = torch.float16                            \n",
    "\n",
    "\n",
    "parser  = PDBParser(QUIET=True)\n",
    "WT_PDB  = \"wt_af.pdb\" # wild-type protein structure in pdb format predicted by AlphaFold2\n",
    "chain   = next(parser.get_structure(\"wt\", WT_PDB).get_chains())\n",
    "aa3_to1 = {'ALA':'A','CYS':'C','ASP':'D','GLU':'E','PHE':'F','GLY':'G','HIS':'H',\n",
    "           'ILE':'I','LYS':'K','LEU':'L','MET':'M','ASN':'N','PRO':'P','GLN':'Q',\n",
    "           'ARG':'R','SER':'S','THR':'T','VAL':'V','TRP':'W','TYR':'Y'}\n",
    "wt_seq  = \"\".join(aa3_to1.get(res.get_resname(), 'X')\n",
    "                  for res in chain.get_residues() if Polypeptide.is_aa(res, True))\n",
    "\n",
    "\n",
    "df = pd.read_pickle(DF_PATH)\n",
    "muts = df[\"mutation\"].unique()              \n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model     = AutoModel.from_pretrained(MODEL_NAME, torch_dtype=dtype).to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "mut_pat = re.compile(r\"([A-Z])(\\d+)([A-Z])\")\n",
    "def apply_mut(seq, mut):\n",
    "    m = mut_pat.fullmatch(mut)\n",
    "    if m is None: raise ValueError(f\"Bad mutation string: {mut}\")\n",
    "    pos = int(m.group(2)) - 1          \n",
    "    aa_new = m.group(3)\n",
    "    return seq[:pos] + aa_new + seq[pos+1:]\n",
    "\n",
    "\n",
    "emb_bank = {}\n",
    "with torch.inference_mode():\n",
    "    for i in tqdm(range(0, len(muts), BATCH_SIZE), ncols=88,\n",
    "                  desc=\"Embedding mutants\"):\n",
    "        batch_muts = muts[i:i+BATCH_SIZE]\n",
    "        seqs = [apply_mut(wt_seq, m) for m in batch_muts]\n",
    "        toks = tokenizer(seqs, return_tensors=\"pt\", padding=True)\n",
    "        toks = {k: v.to(device) for k, v in toks.items()}\n",
    "        out  = model(**toks, output_hidden_states=False).last_hidden_state\n",
    "\n",
    "        # Remove BOS / EOS\n",
    "        for mut, emb, length in zip(batch_muts, out, toks[\"attention_mask\"].sum(1)-2):\n",
    "            emb = emb[1:length+1].cpu().to(torch.float16).numpy() \n",
    "            emb_bank[mut] = emb\n",
    "\n",
    "np.savez_compressed(OUT_PATH, **emb_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN + ESM embedding model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "from scipy.spatial import KDTree\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from Bio.PDB import PDBParser, Polypeptide\n",
    "\n",
    "OPTIMAL_RADIUS = 8.0\n",
    "OPTIMAL_HIDDEN_DIM = 96\n",
    "OPTIMAL_DROPOUT = 0.4\n",
    "\n",
    "SEED = 42\n",
    "N_SPLITS = 5\n",
    "BATCH_SIZE = 16\n",
    "MAX_EPOCHS = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "EARLY_STOPPING_PATIENCE = 15\n",
    "\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "\n",
    "mutation_data_path = 'mutations_full_info.pkl' # data stored in pickle format, with columns: mutation, scaled_activity, group, sequence\n",
    "mutations = pd.read_pickle(mutation_data_path)\n",
    "\n",
    "y = mutations[\"scaled_activity\"].to_numpy(dtype=np.float32)\n",
    "groups = mutations[\"mutation\"].str.extract(r\"[A-Z](\\d+)[A-Z]\").astype(int)[0].to_numpy()\n",
    "\n",
    "esm_embeddings_path = 'mutant_embeddings.npz' # ESM-2 embeddings of mutants, generated above\n",
    "esm_bank = np.load(esm_embeddings_path)\n",
    "\n",
    "\n",
    "def custom_three_to_one(residue_name: str) -> str:\n",
    "    _3to1_map = {'ALA': 'A', 'CYS': 'C', 'ASP': 'D', 'GLU': 'E', 'PHE': 'F', 'GLY': 'G', 'HIS': 'H', 'ILE': 'I', 'LYS': 'K', 'LEU': 'L', 'MET': 'M', 'ASN': 'N', 'PRO': 'P', 'GLN': 'Q', 'ARG': 'R', 'SER': 'S', 'THR': 'T', 'VAL': 'V', 'TRP': 'W', 'TYR': 'Y'}\n",
    "    return _3to1_map.get(residue_name.upper(), 'X')\n",
    "\n",
    "WT_PDB_PATH = \"wt_af.pdb\" # wild-type protein structure in pdb format predicted by AlphaFold2\n",
    "parser = PDBParser(QUIET=True)\n",
    "structure = parser.get_structure(\"wt\", WT_PDB_PATH)\n",
    "chain = next(structure.get_chains())\n",
    "residues = [res for res in chain.get_residues() if Polypeptide.is_aa(res, standard=True)]\n",
    "coords = np.array([res[\"CA\"].get_coord() for res in residues], dtype=np.float32)\n",
    "wt_seq = \"\".join(custom_three_to_one(res.get_resname()) for res in residues)\n",
    "\n",
    "AA_CODES = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "def aa_onehot(code):\n",
    "    vec = np.zeros(len(AA_CODES), dtype=np.float32)\n",
    "    vec[AA_CODES.index(code)] = 1.0\n",
    "    return vec\n",
    "\n",
    "all_node_features = []\n",
    "mut_pat = re.compile(r\"([A-Z])(\\d+)([A-Z])\")\n",
    "mut_indices = []\n",
    "\n",
    "for _, row in mutations.iterrows():\n",
    "    mut = row[\"mutation\"]\n",
    "    match = mut_pat.fullmatch(mut)\n",
    "    if not match: continue\n",
    "    \n",
    "    wt, pos, mut_aa = match.groups()\n",
    "    idx = int(pos) - 1\n",
    "    mut_indices.append(idx)\n",
    "\n",
    "    esm_embedding = esm_bank[mut]\n",
    "    \n",
    "    mutation_indicator = np.zeros((len(wt_seq), 1), dtype=np.float32)\n",
    "    mutation_indicator[idx] = 1.0\n",
    "    mut_aa_onehot = np.zeros((len(wt_seq), len(AA_CODES)), dtype=np.float32)\n",
    "    mut_aa_onehot[idx] = aa_onehot(mut_aa)\n",
    "\n",
    "    combined_features = np.hstack([esm_embedding, mutation_indicator, mut_aa_onehot])\n",
    "    all_node_features.append(combined_features)\n",
    "\n",
    "\n",
    "def create_radius_adjacency_matrix(coords, radius=10.0):\n",
    "    tree = KDTree(coords)\n",
    "    pairs = tree.query_pairs(r=radius)\n",
    "    row_indices, col_indices = zip(*pairs) if pairs else ([], [])\n",
    "    all_rows = np.concatenate([row_indices, col_indices, np.arange(len(coords))])\n",
    "    all_cols = np.concatenate([col_indices, row_indices, np.arange(len(coords))])\n",
    "    adj = coo_matrix((np.ones(len(all_rows)), (all_rows, all_cols)), shape=(len(coords), len(coords)))\n",
    "    return adj.tocsr()\n",
    "\n",
    "def get_device():\n",
    "    if torch.backends.mps.is_available(): return \"mps\"\n",
    "    if torch.cuda.is_available(): return \"cuda\"\n",
    "    return \"cpu\"\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout=0.2, alpha=0.2):\n",
    "        super().__init__()\n",
    "        self.dropout, self.in_features, self.out_features, self.alpha = dropout, in_features, out_features, alpha\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.zeros(size=(2 * out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
    "\n",
    "    def forward(self, h, adj):\n",
    "        Wh = torch.mm(h, self.W)\n",
    "        a_input = self._prepare_attentional_mechanism_input(Wh)\n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
    "        zero_vec = -9e15 * torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.matmul(attention, Wh)\n",
    "        return F.elu(h_prime)\n",
    "\n",
    "    def _prepare_attentional_mechanism_input(self, Wh):\n",
    "        N = Wh.size()[0]\n",
    "        Wh_repeated_in_chunks = Wh.repeat_interleave(N, dim=0)\n",
    "        Wh_repeated_alternating = Wh.repeat(N, 1)\n",
    "        all_combinations_matrix = torch.cat([Wh_repeated_in_chunks, Wh_repeated_alternating], dim=1)\n",
    "        return all_combinations_matrix.view(N, N, 2 * self.out_features)\n",
    "\n",
    "class MutGraphNet(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.gat1 = GraphAttentionLayer(in_dim, hidden_dim, dropout=dropout)\n",
    "        self.gat2 = GraphAttentionLayer(hidden_dim, hidden_dim, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x, adj, mut_idx):\n",
    "        x = self.gat1(x, adj)\n",
    "        x = self.dropout(x)\n",
    "        x = self.gat2(x, adj)\n",
    "        x = x[mut_idx]\n",
    "        return self.fc(x)\n",
    "\n",
    "class ProteinGraphDataset(Dataset):\n",
    "    def __init__(self, features_list, labels, mut_indices):\n",
    "        self.features_list = features_list\n",
    "        self.labels = labels\n",
    "        self.mut_indices = mut_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.features_list[idx], dtype=torch.float32),\n",
    "            torch.tensor(self.labels[idx], dtype=torch.float32),\n",
    "            self.mut_indices[idx]\n",
    "        )\n",
    "\n",
    "\n",
    "def train_and_evaluate_gnn(features, labels, groups, mut_indices, adj, params, patience):\n",
    "    kf = GroupKFold(n_splits=N_SPLITS)\n",
    "    oof_preds = np.zeros(len(labels))\n",
    "    device = get_device()\n",
    "\n",
    "    \n",
    "    adj_torch = torch.from_numpy(adj.todense().astype(np.float32)).to(device)\n",
    "\n",
    "    fold_metrics_history = {\n",
    "        'rmse': [], 'r2': [], 'pearson': [], 'spearman': []\n",
    "    }\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(features, labels, groups)):\n",
    "\n",
    "        \n",
    "        model = MutGraphNet(\n",
    "            in_dim=features[0].shape[1], \n",
    "            hidden_dim=params['hidden_dim'], \n",
    "            dropout=params['dropout']\n",
    "        ).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'], weight_decay=1e-5)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5)\n",
    "        \n",
    "        train_dataset = ProteinGraphDataset(\n",
    "            [features[i] for i in train_idx], labels[train_idx], [mut_indices[i] for i in train_idx]\n",
    "        )\n",
    "        val_dataset = ProteinGraphDataset(\n",
    "            [features[i] for i in val_idx], labels[val_idx], [mut_indices[i] for i in val_idx]\n",
    "        )\n",
    "        loader_tr = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        loader_va = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        best_model_path = f\"best_model_fold_{fold+1}.pth\"\n",
    "\n",
    "        for epoch in range(MAX_EPOCHS):\n",
    "            model.train()\n",
    "            for x_batch, y_batch, mut_idx_batch in loader_tr:\n",
    "                for i in range(x_batch.size(0)):\n",
    "                    x, y, mut_idx = x_batch[i].to(device), y_batch[i].to(device), mut_idx_batch[i]\n",
    "                    optimizer.zero_grad()\n",
    "                    output = model(x, adj_torch, mut_idx).squeeze()\n",
    "                    loss = F.mse_loss(output, y)\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    optimizer.step()\n",
    "            \n",
    "            model.eval()\n",
    "            total_val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for x_batch, y_batch, mut_idx_batch in loader_va:\n",
    "                    for i in range(x_batch.size(0)):\n",
    "                        x, y, mut_idx = x_batch[i].to(device), y_batch[i].to(device), mut_idx_batch[i]\n",
    "                        output = model(x, adj_torch, mut_idx).squeeze()\n",
    "                        total_val_loss += F.mse_loss(output, y).item()\n",
    "            avg_val_loss = total_val_loss / len(val_dataset)\n",
    "            \n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss, patience_counter = avg_val_loss, 0\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}.\")\n",
    "                break\n",
    "\n",
    "        model.load_state_dict(torch.load(best_model_path))\n",
    "        model.eval()\n",
    "        \n",
    "        fold_preds_list = []\n",
    "        fold_true_labels = labels[val_idx]\n",
    "        with torch.no_grad():\n",
    "            for x_batch, _, mut_idx_batch in loader_va:\n",
    "                for i in range(x_batch.size(0)):\n",
    "                    x, mut_idx = x_batch[i].to(device), mut_idx_batch[i]\n",
    "                    fold_preds_list.append(model(x, adj_torch, mut_idx).squeeze().cpu().numpy())\n",
    "        \n",
    "        fold_preds = np.array(fold_preds_list).flatten()\n",
    "        oof_preds[val_idx] = fold_preds\n",
    "        \n",
    "        fold_metrics_history['rmse'].append(np.sqrt(mean_squared_error(fold_true_labels, fold_preds)))\n",
    "        fold_metrics_history['r2'].append(r2_score(fold_true_labels, fold_preds))\n",
    "        fold_metrics_history['pearson'].append(pearsonr(fold_true_labels, fold_preds)[0])\n",
    "        fold_metrics_history['spearman'].append(spearmanr(fold_true_labels, fold_preds)[0])\n",
    "        \n",
    "        print(f\"Fold {fold+1} Spearman: {fold_metrics_history['spearman'][-1]:.4f}\")\n",
    "        os.remove(best_model_path)\n",
    "    \n",
    "    overall_metrics = {\n",
    "        'rmse': np.sqrt(mean_squared_error(labels, oof_preds)),\n",
    "        'r2': r2_score(labels, oof_preds),\n",
    "        'pearson': pearsonr(labels, oof_preds)[0],\n",
    "        'spearman': spearmanr(labels, oof_preds)[0]\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'predictions': oof_preds,\n",
    "        'overall_metrics': overall_metrics,\n",
    "        'fold_metrics': fold_metrics_history\n",
    "    }\n",
    "\n",
    "\n",
    "adj_matrix = create_radius_adjacency_matrix(coords, radius=OPTIMAL_RADIUS)\n",
    "model_params = {'lr': LEARNING_RATE, 'hidden_dim': OPTIMAL_HIDDEN_DIM, 'dropout': OPTIMAL_DROPOUT}\n",
    "\n",
    "final_results = train_and_evaluate_gnn(\n",
    "    all_node_features, y, groups, mut_indices,\n",
    "    adj=adj_matrix, params=model_params, patience=EARLY_STOPPING_PATIENCE\n",
    ")\n",
    "\n",
    "for metric_name, metric_values in final_results['fold_metrics'].items():\n",
    "    mean_val = np.mean(metric_values)\n",
    "    std_val = np.std(metric_values)\n",
    "  \n",
    "\n",
    "overall_metrics = final_results['overall_metrics']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN + ESM embedding model (optimized feature engineering)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from functools import partial\n",
    "\n",
    "from scipy.spatial import KDTree\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from Bio.PDB import PDBParser, Polypeptide\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "def get_device():\n",
    "    if torch.backends.mps.is_available(): return torch.device(\"mps\")\n",
    "    if torch.cuda.is_available(): return torch.device(\"cuda\")\n",
    "    return torch.device(\"cpu\")\n",
    "device = get_device()\n",
    "\n",
    "OPTIMAL_RADIUS = 8.0\n",
    "SEED = 42\n",
    "N_SPLITS = 5\n",
    "BATCH_SIZE = 8 \n",
    "MAX_EPOCHS = 50\n",
    "EARLY_STOPPING_PATIENCE = 15\n",
    "\n",
    "hyperparams = {\n",
    "    'lr': 1e-4,         \n",
    "    'hidden_dim': 128,  \n",
    "    'dropout': 0.4,\n",
    "    'lambda_rho': 0.5   \n",
    "}\n",
    "\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "mutation_data_path = 'mutations_full_info.pkl' # data stored in pickle format, with columns: mutation, scaled_activity, group, sequence\n",
    "mutations = pd.read_pickle(mutation_data_path)\n",
    "y_values = mutations[\"scaled_activity\"].to_numpy(dtype=np.float32)\n",
    "groups = mutations[\"mutation\"].str.extract(r\"[A-Z](\\d+)[A-Z]\").astype(int)[0].to_numpy()\n",
    "\n",
    "esm_embeddings_path = 'mutant_embeddings.npz' # ESM-2 embeddings of mutants, generated above\n",
    "esm_bank = np.load(esm_embeddings_path)\n",
    "\n",
    "def custom_three_to_one(residue_name: str) -> str:\n",
    "    _3to1_map = {'ALA': 'A', 'CYS': 'C', 'ASP': 'D', 'GLU': 'E', 'PHE': 'F', 'GLY': 'G', 'HIS': 'H', 'ILE': 'I', 'LYS': 'K', 'LEU': 'L', 'MET': 'M', 'ASN': 'N', 'PRO': 'P', 'GLN': 'Q', 'ARG': 'R', 'SER': 'S', 'THR': 'T', 'VAL': 'V', 'TRP': 'W', 'TYR': 'Y'}\n",
    "    return _3to1_map.get(residue_name.upper(), 'X')\n",
    "\n",
    "WT_PDB_PATH = \"wt_af.pdb\" # wild-type protein structure in pdb format predicted by AlphaFold2\n",
    "parser = PDBParser(QUIET=True)\n",
    "structure = parser.get_structure(\"wt\", WT_PDB_PATH)\n",
    "chain = next(structure.get_chains())\n",
    "residues = [res for res in chain.get_residues() if Polypeptide.is_aa(res, standard=True)]\n",
    "coords = np.array([res[\"CA\"].get_coord() for res in residues], dtype=np.float32)\n",
    "wt_seq_from_pdb = \"\".join(custom_three_to_one(res.get_resname()) for res in residues)\n",
    "\n",
    "WT_EMB_PATH = \"wt_esm_embedding.npz\" # to generate wild-type ESM-2 embedding\n",
    "if not os.path.exists(WT_EMB_PATH):\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
    "    model = AutoModel.from_pretrained(\"facebook/esm2_t33_650M_UR50D\", torch_dtype=torch.float16).to(device)\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        tok = tokenizer(wt_seq_from_pdb, return_tensors=\"pt\").to(device)\n",
    "        wt_emb_tensor = model(**tok).last_hidden_state[0, 1:-1].cpu().to(torch.float16).numpy()\n",
    "    np.savez_compressed(WT_EMB_PATH, emb=wt_emb_tensor)\n",
    "    \n",
    "wt_emb = np.load(WT_EMB_PATH)[\"emb\"]\n",
    "\n",
    "AA_CODES = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "def aa_onehot(code):\n",
    "    vec = np.zeros(len(AA_CODES), dtype=np.float32)\n",
    "    vec[AA_CODES.index(code)] = 1.0\n",
    "    return vec\n",
    "\n",
    "all_node_features = []\n",
    "mut_pat = re.compile(r\"([A-Z])(\\d+)([A-Z])\")\n",
    "mut_indices = []\n",
    "\n",
    "\n",
    "for _, row in mutations.iterrows():\n",
    "    mut = row[\"mutation\"]\n",
    "    match = mut_pat.fullmatch(mut)\n",
    "    if not match: continue\n",
    "\n",
    "    wt, pos, mut_aa = match.groups()\n",
    "    idx = int(pos) - 1\n",
    "    mut_indices.append(idx)\n",
    "\n",
    "    mut_emb = esm_bank[mut]\n",
    "    delta_emb = mut_emb - wt_emb[:mut_emb.shape[0]]\n",
    "\n",
    "    mutation_indicator = np.zeros((len(wt_seq_from_pdb), 1), dtype=np.float32)\n",
    "    mutation_indicator[idx] = 1.0\n",
    "    mut_aa_onehot = np.zeros((len(wt_seq_from_pdb), len(AA_CODES)), dtype=np.float32)\n",
    "    mut_aa_onehot[idx] = aa_onehot(mut_aa)\n",
    "\n",
    "    combined_features = np.hstack([mut_emb, delta_emb, mutation_indicator, mut_aa_onehot])\n",
    "    all_node_features.append(combined_features)\n",
    "    \n",
    "def create_radius_adjacency_matrix(coords, radius=10.0):\n",
    "    tree = KDTree(coords)\n",
    "    pairs = tree.query_pairs(r=radius)\n",
    "    row_indices, col_indices = zip(*pairs) if pairs else ([], [])\n",
    "    all_rows = np.concatenate([row_indices, col_indices, np.arange(len(coords))])\n",
    "    all_cols = np.concatenate([col_indices, row_indices, np.arange(len(coords))])\n",
    "    adj = coo_matrix((np.ones(len(all_rows)), (all_rows, all_cols)), shape=(len(coords), len(coords)))\n",
    "    return adj.tocsr()\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout=0.2, alpha=0.2):\n",
    "        super().__init__()\n",
    "        self.dropout, self.in_features, self.out_features, self.alpha = dropout, in_features, out_features, alpha\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.zeros(size=(2 * out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
    "\n",
    "    def forward(self, h, adj):\n",
    "        Wh = torch.mm(h, self.W)\n",
    "        a_input = self._prepare_attentional_mechanism_input(Wh)\n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
    "        zero_vec = -9e15 * torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.matmul(attention, Wh)\n",
    "        return F.elu(h_prime)\n",
    "\n",
    "    def _prepare_attentional_mechanism_input(self, Wh):\n",
    "        N = Wh.size()[0]\n",
    "        Wh_repeated_in_chunks = Wh.repeat_interleave(N, dim=0)\n",
    "        Wh_repeated_alternating = Wh.repeat(N, 1)\n",
    "        all_combinations_matrix = torch.cat([Wh_repeated_in_chunks, Wh_repeated_alternating], dim=1)\n",
    "        return all_combinations_matrix.view(N, N, 2 * self.out_features)\n",
    "\n",
    "class MutGraphNet(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.gat1 = GraphAttentionLayer(in_dim, hidden_dim, dropout=dropout)\n",
    "        self.gat2 = GraphAttentionLayer(hidden_dim, hidden_dim, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x, adj, mut_idx):\n",
    "        x = self.gat1(x, adj)\n",
    "        x = self.dropout(x)\n",
    "        x = self.gat2(x, adj)\n",
    "        x = x[mut_idx]\n",
    "        return self.fc(x)\n",
    "\n",
    "class ProteinGraphDataset(Dataset):\n",
    "    def __init__(self, features_list, labels, mut_indices):\n",
    "        self.features_list = features_list\n",
    "        self.labels = labels\n",
    "        self.mut_indices = mut_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (torch.tensor(self.features_list[idx], dtype=torch.float32),\n",
    "                torch.tensor(self.labels[idx], dtype=torch.float32),\n",
    "                self.mut_indices[idx])\n",
    "\n",
    "def soft_rank_pytorch(x, steepness=1.0):\n",
    "    diff = x.unsqueeze(1) - x.unsqueeze(0)\n",
    "    return torch.sum(torch.sigmoid(steepness * diff), dim=1)\n",
    "\n",
    "def spearman_soft_pytorch(pred, target, steepness=1.0):\n",
    "    pred_ranks = soft_rank_pytorch(pred, steepness) - 1\n",
    "    target_ranks = soft_rank_pytorch(target, steepness) - 1\n",
    "    pred_ranks_c = pred_ranks - pred_ranks.mean()\n",
    "    target_ranks_c = target_ranks - target_ranks.mean()\n",
    "    pred_norm = torch.linalg.norm(pred_ranks_c)\n",
    "    target_norm = torch.linalg.norm(target_ranks_c)\n",
    "    if pred_norm < 1e-6 or target_norm < 1e-6:\n",
    "        return torch.tensor(0.0, device=pred.device, requires_grad=True)\n",
    "    return (pred_ranks_c * target_ranks_c).sum() / (pred_norm * target_norm)\n",
    "\n",
    "huber_loss_fn = nn.SmoothL1Loss(beta=1.0)\n",
    "def huber_rho_loss(pred, target, lambda_rho, steepness=1.0):\n",
    "    huber_part = huber_loss_fn(pred, target)\n",
    "    rho_part = 1.0 - spearman_soft_pytorch(pred, target, steepness)\n",
    "    return huber_part + lambda_rho * rho_part\n",
    "\n",
    "def train_and_evaluate_gnn(features, labels, groups, mut_indices_list, adj, params, patience):\n",
    "    kf = GroupKFold(n_splits=N_SPLITS)\n",
    "    oof_preds = np.zeros(len(labels))\n",
    "    adj_torch = torch.from_numpy(adj.todense().astype(np.float32)).to(device)\n",
    "    fold_metrics_history = {'rmse': [], 'r2': [], 'pearson': [], 'spearman': []}\n",
    "    loss_fn = partial(huber_rho_loss, lambda_rho=params['lambda_rho'])\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(features, labels, groups)):\n",
    "        \n",
    "        model = MutGraphNet(in_dim=features[0].shape[1], hidden_dim=params['hidden_dim'], dropout=params['dropout']).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'], weight_decay=1e-5)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5)\n",
    "        \n",
    "        train_ds = ProteinGraphDataset([features[i] for i in train_idx], labels[train_idx], [mut_indices_list[i] for i in train_idx])\n",
    "        val_ds = ProteinGraphDataset([features[i] for i in val_idx], labels[val_idx], [mut_indices_list[i] for i in val_idx])\n",
    "        loader_tr = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        loader_va = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "        best_val_loss, patience_counter = float('inf'), 0\n",
    "        best_model_path = f\"best_model_fold_{fold+1}.pth\"\n",
    "\n",
    "        for epoch in range(MAX_EPOCHS):\n",
    "            model.train()\n",
    "            for x_batch, y_batch, mut_idx_batch in loader_tr:\n",
    "                optimizer.zero_grad()\n",
    "                batch_outputs = [model(x.to(device), adj_torch, mut_idx).squeeze() for x, mut_idx in zip(x_batch, mut_idx_batch)]\n",
    "                outputs_tensor = torch.stack(batch_outputs)\n",
    "                loss = loss_fn(outputs_tensor, y_batch.to(device))\n",
    "                if torch.isnan(loss): continue\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            val_outputs_list = []\n",
    "            with torch.no_grad():\n",
    "                for x_batch, _, mut_idx_batch in loader_va:\n",
    "                    for i in range(x_batch.size(0)):\n",
    "                        x, mut_idx = x_batch[i].to(device), mut_idx_batch[i]\n",
    "                        output = model(x, adj_torch, mut_idx).squeeze()\n",
    "                        val_outputs_list.append(output.cpu())\n",
    "\n",
    "            val_outputs_tensor = torch.stack(val_outputs_list)\n",
    "            val_labels_tensor = torch.tensor(val_ds.labels, dtype=torch.float32)\n",
    "            \n",
    "            avg_val_loss = loss_fn(val_outputs_tensor.to(device), val_labels_tensor.to(device)).item()\n",
    "            val_outputs_np = val_outputs_tensor.numpy()\n",
    "            val_labels_np = val_labels_tensor.numpy()\n",
    "            val_spearman = spearmanr(val_labels_np, val_outputs_np)[0]\n",
    "            print(f\"epoch {epoch+1:02d}/{MAX_EPOCHS} | val loss: {avg_val_loss:.4f} | val spearman: {val_spearman:.4f}\")\n",
    "\n",
    "            scheduler.step(avg_val_loss)\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss, patience_counter = avg_val_loss, 0\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}.\")\n",
    "                break\n",
    "        \n",
    "        model.load_state_dict(torch.load(best_model_path))\n",
    "        model.eval()\n",
    "        fold_preds_list = []\n",
    "        with torch.no_grad():\n",
    "            for x_batch, _, mut_idx_batch in loader_va:\n",
    "                for i in range(x_batch.size(0)):\n",
    "                    x, mut_idx = x_batch[i].to(device), mut_idx_batch[i]\n",
    "                    output = model(x, adj_torch, mut_idx).squeeze()\n",
    "                    fold_preds_list.append(output.cpu())\n",
    "        \n",
    "        fold_preds = torch.stack(fold_preds_list).numpy()\n",
    "        oof_preds[val_idx] = fold_preds\n",
    "        fold_true = labels[val_idx]\n",
    "\n",
    "        fold_metrics_history['rmse'].append(np.sqrt(mean_squared_error(fold_true, fold_preds)))\n",
    "        fold_metrics_history['r2'].append(r2_score(fold_true, fold_preds))\n",
    "        fold_metrics_history['pearson'].append(pearsonr(fold_true, fold_preds)[0])\n",
    "        fold_metrics_history['spearman'].append(spearmanr(fold_true, fold_preds)[0])\n",
    "        print(f\"Fold {fold+1} Spearman: {fold_metrics_history['spearman'][-1]:.4f}\")\n",
    "        os.remove(best_model_path)\n",
    "\n",
    "    overall_metrics = {'rmse': np.sqrt(mean_squared_error(labels, oof_preds)), 'r2': r2_score(labels, oof_preds), 'pearson': pearsonr(labels, oof_preds)[0], 'spearman': spearmanr(labels, oof_preds)[0]}\n",
    "    return {'predictions': oof_preds, 'overall_metrics': overall_metrics, 'fold_metrics': fold_metrics_history}\n",
    "\n",
    "\n",
    "adj_matrix = create_radius_adjacency_matrix(coords, radius=OPTIMAL_RADIUS)\n",
    "final_results = train_and_evaluate_gnn(all_node_features, y_values, groups, mut_indices, adj=adj_matrix, params=hyperparams, patience=EARLY_STOPPING_PATIENCE)\n",
    "\n",
    "\n",
    "for metric_name, metric_values in final_results['fold_metrics'].items():\n",
    "    mean_val, std_val = np.mean(metric_values), np.std(metric_values)\n",
    "\n",
    "overall_metrics = final_results['overall_metrics']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EGNN + ESM embedding model (optimized feature engineering)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import warnings\n",
    "import torch\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "from Bio.PDB import PDBParser, Polypeptide\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "mutations = pd.read_pickle('mutations_full_info.pkl') # data stored in pickle format, with columns: mutation, scaled_activity, group, sequence\n",
    "\n",
    "y_values = mutations[\"scaled_activity\"].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    "groups = mutations[\"mutation\"].str.extract(r\"[A-Z](\\d+)[A-Z]\").astype(int)[0].to_numpy()\n",
    "\n",
    "def get_device():\n",
    "    if torch.backends.mps.is_available(): return torch.device(\"mps\")\n",
    "    if torch.cuda.is_available(): return torch.device(\"cuda\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "\n",
    "OUT_PATH = \"mutant_embeddings.npz\" # ESM-2 embeddings of mutants\n",
    "MODEL_NAME = \"facebook/esm2_t33_650M_UR50D\"\n",
    "\n",
    "wt_seq = 'MDSLVVLVLCL...'\n",
    "\n",
    "WT_EMB_PATH = \"wt_esm650m_embedding.npz\" # to generate wild-type ESM-2 embedding\n",
    "if not os.path.exists(WT_EMB_PATH):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModel.from_pretrained(MODEL_NAME, torch_dtype=torch.float16).to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        tok = tokenizer(wt_seq, return_tensors=\"pt\").to(device)\n",
    "        wt_emb = model(**tok).last_hidden_state[0, 1:-1].cpu().to(torch.float16).numpy()\n",
    "    \n",
    "    np.savez_compressed(WT_EMB_PATH, emb=wt_emb)\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "N_SPLITS = 5\n",
    "EPOCHS = 50\n",
    "\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "def scatter_mean_workaround(src, index, dim, dim_size):\n",
    "    out = torch.zeros(dim_size, src.size(1), dtype=src.dtype, device=src.device)\n",
    "    count = torch.zeros(dim_size, 1, dtype=src.dtype, device=src.device)\n",
    "    out.index_add_(dim, index, src)\n",
    "    ones_for_counting = torch.ones(src.size(0), 1, dtype=src.dtype, device=src.device)\n",
    "    count.index_add_(dim, index, ones_for_counting)\n",
    "    return out / count.clamp(min=1)\n",
    "\n",
    "WT_PDB = \"wt_af.pdb\"\n",
    "parser = PDBParser(QUIET=True)\n",
    "structure = parser.get_structure(\"wt\", WT_PDB)\n",
    "chain = next(structure.get_chains())\n",
    "residues = [res for res in chain.get_residues() if Polypeptide.is_aa(res, standard=True)]\n",
    "coords = np.array([res[\"CA\"].get_coord() for res in residues], dtype=np.float32)\n",
    "wt_seq_len = len(coords)\n",
    "\n",
    "embedding_bank = np.load(OUT_PATH)\n",
    "wt_emb = np.load(WT_EMB_PATH)[\"emb\"]\n",
    "\n",
    "MUTANT_ESM_DIM = embedding_bank[list(embedding_bank.keys())[0]].shape[1]\n",
    "DELTA_ESM_DIM = wt_emb.shape[1]\n",
    "MUT_INDICATOR_COL_IDX = MUTANT_ESM_DIM + DELTA_ESM_DIM\n",
    "\n",
    "AA_CODES = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "def aa_onehot(code):\n",
    "    vec = np.zeros(len(AA_CODES), dtype=np.float32)\n",
    "    vec[AA_CODES.index(code)] = 1.0\n",
    "    return vec\n",
    "\n",
    "all_node_features = []\n",
    "mut_indices = []\n",
    "mut_pat = re.compile(r\"([A-Z])(\\d+)([A-Z])\")\n",
    "\n",
    "\n",
    "for _, row in mutations.iterrows():\n",
    "    mut_string = row[\"mutation\"]\n",
    "    match = mut_pat.fullmatch(mut_string)\n",
    "    if not match: continue\n",
    "\n",
    "    _, pos, mut_aa = match.groups()\n",
    "    idx = int(pos) - 1\n",
    "\n",
    "    mut_emb = embedding_bank[mut_string]\n",
    "    delta_emb = mut_emb - wt_emb[:mut_emb.shape[0]]\n",
    "    mut_indices.append(idx)\n",
    "    mutation_indicator = np.zeros((wt_seq_len, 1), dtype=np.float16)\n",
    "    mutation_indicator[idx] = 1.0\n",
    "    mut_aa_onehot = np.zeros((wt_seq_len, len(AA_CODES)), dtype=np.float16)\n",
    "    mut_aa_onehot[idx] = aa_onehot(mut_aa)\n",
    "\n",
    "    combined = np.hstack([mut_emb, delta_emb, mutation_indicator, mut_aa_onehot])\n",
    "    all_node_features.append(combined)\n",
    "\n",
    "\n",
    "\n",
    "def create_radius_graph(coords, radius=8.0):\n",
    "    coords_t = torch.from_numpy(coords)\n",
    "    dist_matrix = torch.cdist(coords_t, coords_t)\n",
    "    edge_index = (dist_matrix < radius).nonzero(as_tuple=False).t().contiguous()\n",
    "    return edge_index[:, edge_index[0] != edge_index[1]]\n",
    "\n",
    "GRAPH_RADIUS = 8.0\n",
    "edge_index = create_radius_graph(coords, radius=GRAPH_RADIUS)\n",
    "\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, node_features_list, coords, labels, mut_idx_list):\n",
    "        self.node_features_list = [torch.tensor(f, dtype=torch.float32) for f in node_features_list]\n",
    "        self.coords = torch.tensor(coords, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "        self.mut_idx = mut_idx_list\n",
    "    def __len__(self): return len(self.node_features_list)\n",
    "    def __getitem__(self, idx): \n",
    "        pos = self.coords.clone()\n",
    "        m_i = self.mut_idx[idx]\n",
    "        pos[m_i] = torch.full((3,), float('nan'))\n",
    "        return {\"x\": self.node_features_list[idx], \"pos\": pos, \"y\": self.labels[idx]}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    xs, poss, ys, batch_vecs, edge_indices = [], [], [], [], []\n",
    "    n_nodes_so_far = 0\n",
    "    for i, item in enumerate(batch):\n",
    "        N = item[\"x\"].shape[0]\n",
    "        xs.append(item[\"x\"])\n",
    "        poss.append(item[\"pos\"])\n",
    "        ys.append(item[\"y\"])\n",
    "        batch_vecs.append(torch.full((N,), i, dtype=torch.long))\n",
    "        edge_indices.append(edge_index + n_nodes_so_far)\n",
    "        n_nodes_so_far += N\n",
    "    return {\n",
    "        \"x\": torch.cat(xs, dim=0), \"pos\": torch.cat(poss, dim=0),\n",
    "        \"edge_index\": torch.cat(edge_indices, dim=1), \"batch\": torch.cat(batch_vecs, dim=0),\n",
    "        \"y\": torch.stack(ys).squeeze(-1)}\n",
    "\n",
    "dataset = ProteinDataset(all_node_features, coords, y_values, mut_indices)\n",
    "\n",
    "\n",
    "class E_GCL(nn.Module): # equivariant graph convolution layer\n",
    "    def __init__(self, input_nf, output_nf, hidden_nf, act_fn=nn.SiLU(), residual=True, num_rbf=16, rbf_cutoff=12.0):\n",
    "        super().__init__()\n",
    "        self.residual = residual\n",
    "        self.edge_mlp = nn.Sequential(nn.Linear(input_nf * 2 + num_rbf, hidden_nf), act_fn, nn.Linear(hidden_nf, hidden_nf), act_fn)\n",
    "        self.node_mlp = nn.Sequential(nn.Linear(hidden_nf + input_nf, hidden_nf), act_fn, nn.Linear(hidden_nf, output_nf))\n",
    "        self.coord_mlp = nn.Sequential(nn.Linear(hidden_nf, hidden_nf), act_fn, nn.Linear(hidden_nf, 1, bias=False))\n",
    "        centres = torch.linspace(0.0, rbf_cutoff, num_rbf)\n",
    "        gamma = (rbf_cutoff / num_rbf) ** -2\n",
    "        self.register_buffer(\"rbf_centres\", centres)\n",
    "        self.register_buffer(\"rbf_gamma\", torch.full((num_rbf,), gamma))\n",
    "    def edge_model(self, source, target, rbf): return self.edge_mlp(torch.cat([source, target, rbf], dim=1))\n",
    "    def node_model(self, x, edge_index, edge_attr):\n",
    "        row, _ = edge_index\n",
    "        agg = scatter_mean_workaround(edge_attr, row, dim=0, dim_size=x.size(0))\n",
    "        return self.node_mlp(torch.cat([x, agg], dim=1))\n",
    "    def coord_model(self, coord, edge_index, coord_diff, edge_feat):\n",
    "        row, _ = edge_index\n",
    "        trans = coord_diff * self.coord_mlp(edge_feat)\n",
    "        return scatter_mean_workaround(trans, row, dim=0, dim_size=coord.size(0))\n",
    "    def forward(self, h, edge_index, coord):\n",
    "        coord_filled = torch.nan_to_num(coord, nan=0.0)\n",
    "        row, col = edge_index\n",
    "        coord_diff = coord_filled[row] - coord_filled[col]\n",
    "        dist = torch.norm(coord_diff, dim=1, keepdim=True)\n",
    "        rbf = torch.exp(-self.rbf_gamma * (dist - self.rbf_centres) ** 2)\n",
    "        edge_feat = self.edge_model(h[row], h[col], rbf)\n",
    "        delta = self.coord_model(coord_filled, edge_index, coord_diff, edge_feat)\n",
    "        coord = torch.where(torch.isnan(coord), delta, coord + delta)\n",
    "        h_new = self.node_model(h, edge_index, edge_feat)\n",
    "        h = h + h_new if self.residual else h_new\n",
    "        return h, coord\n",
    "\n",
    "class MutEGNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_layers, dropout, mut_col: int):\n",
    "        super().__init__()\n",
    "        self.mut_col = mut_col\n",
    "        self.embed = nn.Linear(in_dim, hidden_dim)\n",
    "        self.layers = nn.ModuleList([E_GCL(hidden_dim, hidden_dim, hidden_dim) for _ in range(n_layers)])\n",
    "        self.head = nn.Sequential(nn.Linear(hidden_dim, hidden_dim * 2), nn.SiLU(), nn.Dropout(dropout), nn.Linear(hidden_dim * 2, 1))\n",
    "    def forward(self, data_batch, edge_index):\n",
    "        x, pos, batch_vec = data_batch['x'], data_batch['pos'], data_batch['batch']\n",
    "        mut_mask = x[:, self.mut_col:self.mut_col + 1]\n",
    "        h = self.embed(x)\n",
    "        for layer in self.layers:\n",
    "            h, pos = layer(h, edge_index, pos)\n",
    "        mean_weighted_h = scatter_mean_workaround(h * mut_mask, batch_vec, dim=0, dim_size=data_batch['y'].size(0))\n",
    "        mean_mask = scatter_mean_workaround(mut_mask, batch_vec, dim=0, dim_size=data_batch['y'].size(0))\n",
    "        graph_features = mean_weighted_h / (mean_mask + 1e-6)\n",
    "        return self.head(graph_features).squeeze(-1)\n",
    "\n",
    "def soft_rank_pytorch(x, steepness=1.0):\n",
    "    diff = x.unsqueeze(1) - x.unsqueeze(0)\n",
    "    ranks = torch.sum(torch.sigmoid(steepness * diff), dim=1)\n",
    "    return ranks\n",
    "\n",
    "def spearman_soft_pytorch(pred, target, steepness=1.0):\n",
    "    pred_ranks = soft_rank_pytorch(pred, steepness)\n",
    "    target_ranks = soft_rank_pytorch(target, steepness)\n",
    "    \n",
    "    pred_ranks = pred_ranks - pred_ranks.mean()\n",
    "    target_ranks = target_ranks - target_ranks.mean()\n",
    "\n",
    "    pred_norm = torch.linalg.norm(pred_ranks)\n",
    "    target_norm = torch.linalg.norm(target_ranks)\n",
    "\n",
    "    if pred_norm < 1e-6 or target_norm < 1e-6:\n",
    "        return torch.tensor(0.0, device=pred.device, requires_grad=True)\n",
    "    \n",
    "    return (pred_ranks * target_ranks).sum() / (pred_norm * target_norm)\n",
    "\n",
    "huber_loss_fn = nn.SmoothL1Loss(beta=1.0)\n",
    "\n",
    "def huber_rho_loss(pred, target, lambda_rho, steepness=1.0):\n",
    "    huber_part = huber_loss_fn(pred, target)\n",
    "    rho_part = 1.0 - spearman_soft_pytorch(pred, target, steepness)\n",
    "    return huber_part + lambda_rho * rho_part\n",
    "\n",
    "def train_evaluate_egnn(dataset, groups, edge_index_template, params, mut_indicator_col):\n",
    "    kf = GroupKFold(n_splits=N_SPLITS)\n",
    "    oof_preds, true_labels = np.zeros(len(dataset)), np.zeros(len(dataset))\n",
    "    device = get_device()\n",
    "\n",
    "    \n",
    "    fold_metrics_history = {'rmse': [], 'r2': [], 'pearson': [], 'spearman': []}\n",
    "    edge_index = edge_index_template.to(device)\n",
    "\n",
    "    loss_fn = partial(huber_rho_loss, lambda_rho=params['lambda_rho'])\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(dataset, groups=groups)):\n",
    "\n",
    "        train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
    "        val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
    "        loader_tr = DataLoader(train_subset, batch_size=params['batch_size'], shuffle=True, collate_fn=collate_fn)\n",
    "        loader_va = DataLoader(val_subset, batch_size=params['batch_size'], collate_fn=collate_fn)\n",
    "        \n",
    "        model = MutEGNN(\n",
    "            in_dim=dataset[0]['x'].shape[1],\n",
    "            hidden_dim=params['hidden_dim'],\n",
    "            n_layers=params['n_layers'],\n",
    "            dropout=params['dropout'],\n",
    "            mut_col=mut_indicator_col\n",
    "        ).to(device)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'], weight_decay=1e-5)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5)\n",
    "        \n",
    "        best_val_loss, epochs_no_improve = float('inf'), 0\n",
    "        patience = params.get('patience', 15)\n",
    "        best_model_path = f\"best_model_fold_{fold+1}.pth\"\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            model.train()\n",
    "            for batch in loader_tr:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                optimizer.zero_grad()\n",
    "                output = model(batch, batch[\"edge_index\"])\n",
    "                loss = loss_fn(output, batch['y'])\n",
    "                \n",
    "                if torch.isnan(loss):\n",
    "                    warnings.warn(f\"NaN loss detected in fold {fold+1}, epoch {epoch+1}. skip current batch.\")\n",
    "                    continue\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "            \n",
    "            model.eval()\n",
    "            total_val_loss, fold_preds_val, fold_true_val = 0, [], []\n",
    "            with torch.no_grad():\n",
    "                for batch in loader_va:\n",
    "                    batch_device = {k: v.to(device) for k, v in batch.items()}\n",
    "                    preds = model(batch_device, batch_device['edge_index'])\n",
    "                    if not torch.all(torch.isfinite(preds)):\n",
    "                         continue\n",
    "                    \n",
    "                    val_loss = loss_fn(preds, batch_device['y'])\n",
    "                    if not torch.isnan(val_loss):\n",
    "                        total_val_loss += val_loss.item() * batch_device['y'].size(0)\n",
    "                        fold_preds_val.extend(preds.cpu().numpy().flatten())\n",
    "                        fold_true_val.extend(batch['y'].cpu().numpy().flatten())\n",
    "\n",
    "            if len(fold_true_val) == 0:\n",
    "                print(f\"All validation batches were unstable in fold {fold+1}, epoch {epoch+1}. skip current epoch.\")\n",
    "                continue\n",
    "\n",
    "            avg_val_loss = total_val_loss / len(fold_true_val)\n",
    "            val_spearman = spearmanr(fold_true_val, fold_preds_val)[0]\n",
    "            print(f\"epoch {epoch+1:02d}/{EPOCHS} | val loss: {avg_val_loss:.4f} | val spearman: {val_spearman:.4f}\")\n",
    "            \n",
    "            scheduler.step(avg_val_loss)\n",
    "            \n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss; epochs_no_improve = 0\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "            \n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}!\"); break\n",
    "        \n",
    "        \n",
    "        model.load_state_dict(torch.load(best_model_path))\n",
    "        \n",
    "        model.eval()\n",
    "        fold_preds, fold_true = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in loader_va:\n",
    "                batch_device = {k: v.to(device) for k, v in batch.items()}\n",
    "                preds = model(batch_device, batch_device['edge_index'])\n",
    "                if torch.all(torch.isfinite(preds)):\n",
    "                    fold_preds.extend(preds.cpu().numpy().flatten()); fold_true.extend(batch['y'].cpu().numpy().flatten())\n",
    "        \n",
    "        if len(fold_true) > 1:\n",
    "            oof_preds[val_idx], true_labels[val_idx] = fold_preds, fold_true\n",
    "            fold_metrics_history['rmse'].append(np.sqrt(mean_squared_error(fold_true, fold_preds)))\n",
    "            fold_metrics_history['r2'].append(r2_score(fold_true, fold_preds))\n",
    "            fold_metrics_history['pearson'].append(pearsonr(fold_true, fold_preds)[0])\n",
    "            fold_metrics_history['spearman'].append(spearmanr(fold_true, fold_preds)[0])\n",
    "            print(f\"Fold {fold+1} Spearman: {fold_metrics_history['spearman'][-1]:.4f}\")\n",
    "        \n",
    "        if os.path.exists(best_model_path): os.remove(best_model_path)\n",
    "        \n",
    "    overall_metrics = {\n",
    "        'rmse': np.sqrt(mean_squared_error(true_labels, oof_preds)),\n",
    "        'r2': r2_score(true_labels, oof_preds),\n",
    "        'pearson': pearsonr(true_labels, oof_preds)[0],\n",
    "        'spearman': spearmanr(true_labels, oof_preds)[0]\n",
    "    }\n",
    "    \n",
    "    return {'overall_metrics': overall_metrics, 'fold_metrics': fold_metrics_history, 'oof_predictions': {'true': true_labels, 'pred': oof_preds}}\n",
    "\n",
    "hyperparams = {\n",
    "    'lr': 1e-4,\n",
    "    'batch_size': 16,\n",
    "    'hidden_dim': 128,\n",
    "    'n_layers': 3,\n",
    "    'dropout': 0.2,\n",
    "    'patience': 15,\n",
    "    'lambda_rho': 0.5\n",
    "}\n",
    "\n",
    "final_results = train_evaluate_egnn(dataset, groups, edge_index, params=hyperparams, mut_indicator_col=MUT_INDICATOR_COL_IDX)\n",
    "\n",
    "for metric_name, metric_values in final_results['fold_metrics'].items():\n",
    "    mean_val, std_val = np.mean(metric_values), np.std(metric_values)\n",
    "    \n",
    "overall_metrics = final_results['overall_metrics']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openfold-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
